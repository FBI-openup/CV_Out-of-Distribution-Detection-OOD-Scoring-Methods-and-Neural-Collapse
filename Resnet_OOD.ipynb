{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3905f231",
   "metadata": {},
   "source": [
    "Practical Work: Out-of-Distribution Detection, OOD Scoring Methods, and Neural Collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee53c4b",
   "metadata": {},
   "source": [
    "# 1. Training a ResNet18 classifier on CIFAR-100 with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bf77ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd94a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79ec681c5df0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "num_workers = 8\n",
    "lr = 1e-3\n",
    "momentum = 0.95\n",
    "weight_decay = 5e-4\n",
    "epochs = 200\n",
    "\n",
    "# Skip training and load model?\n",
    "skip_training = False  # True to skip\n",
    "\n",
    "# Random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca40df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    transforms.RandomErasing(p=0.25),  # 增加RandomErasing概率\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load CIFAR-100\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Model configuration\n",
    "model = resnet18(False)\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.5),  # 增加Dropout从0.3到0.5以减少过拟合\n",
    "    torch.nn.Linear(model.fc.in_features, 100)\n",
    ")\n",
    "model = model.cuda()\n",
    "\n",
    "# Optim, Loss, Scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch}: Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Simple Test Loop\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f\"Test Accuracy: {epoch_acc:.2f}%\")\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcfabd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model\n",
    "if skip_training:  # Change to True to load the model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('resnet18_cifar100.pth', map_location=device))\n",
    "        # plot_path = \"training_curves.png\"\n",
    "        # img = plt.imread(plot_path)\n",
    "        # plt.figure(figsize=(12, 5))\n",
    "        # plt.imshow(img)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "    except FileNotFoundError:\n",
    "        assert False, \"Model or training curves not found. Please train the model first.\"\n",
    "else:\n",
    "    # Main training and testing loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(epoch)\n",
    "        test_loss, test_acc = test(epoch)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'resnet18_cifar100.pth')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(test_accs, label='Test Acc')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ae369",
   "metadata": {},
   "source": [
    "# 2. Implement and compare OOD scores\n",
    "Implement and compare the following OOD scores:\n",
    "\n",
    "· Max Softmax Probability (MSP)\n",
    "\n",
    "· Maximum Logit Score\n",
    "\n",
    "· Mahalanobis\n",
    "\n",
    "· Energy Score\n",
    "\n",
    "· ViM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45bdb9",
   "metadata": {},
   "source": [
    "# 2.1 Max Softmax Probability (MSP). \n",
    "If the classifier assigns low maximum probability, it is unsure or the input may be OOD.\n",
    "\n",
    "Score: the maximum softmax probability\n",
    "$$s_{max-prob}(x)=\\mathop{max}\\limits_{c} p(y=c|x)=\\mathop{max}\\limits_{c} softmax(z_c(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ca85f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSP scores: tensor([0.2913, 0.3016, 0.4502,  ..., 1.0000, 0.4367, 0.3209])\n"
     ]
    }
   ],
   "source": [
    "def msp_score(model, loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            max_probs, _ = torch.max(probs, dim=1)\n",
    "            \n",
    "            all_scores.append(max_probs.cpu())\n",
    "            \n",
    "    return torch.cat(all_scores)\n",
    "\n",
    "msp_scores = msp_score(model, test_loader)\n",
    "\n",
    "print(f\"MSP scores: {msp_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed7d6",
   "metadata": {},
   "source": [
    "# 3. Max Logit technique. \n",
    "Logits reflect raw model evidence before the softmax normalization; using logits avoids saturating effects of softmax.\n",
    "$$s_{max-logit}(x)= \\mathop{max}\\limits_{c}\\ z_c(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b1af854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max logit scores: tensor([ 6.6087,  5.7924,  8.6517,  ..., 18.1743,  7.1058,  6.6317])\n"
     ]
    }
   ],
   "source": [
    "def max_logit_score(model, loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs)\n",
    "            max_logits, _ = torch.max(logits, dim=1)\n",
    "            all_scores.append(max_logits.cpu())\n",
    "    return torch.cat(all_scores)\n",
    "\n",
    "max_logit_scores = max_logit_score(model, test_loader)\n",
    "print(f\"Max logit scores: {max_logit_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae6a9e",
   "metadata": {},
   "source": [
    "# 4. Energy-based OOD score.\n",
    "\n",
    "Energy provides a scalar that correlates with the model’s\n",
    "total evidence across classes. Lower energy (more negative) implies\n",
    "stronger evidence; higher energy (less negative or positive) can\n",
    "indicate OOD. Energy score derived from the logits: a common definition:\n",
    "\n",
    "$$  E(x) = -log(\\sum_{c}{ }e^{z_c(x)}) = -LSE(z(x))$$\n",
    "\n",
    "With temperature $T>0$ one can use:\n",
    "$$ E_T(x) = -T*log(\\sum_{c}{ }e^{z_c(x)/T})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f2058a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy scores: tensor([ 7.8421,  6.9911,  9.4498,  ..., 18.1744,  7.9342,  7.7682])\n"
     ]
    }
   ],
   "source": [
    "def energy_score(model, loader, Temperature=1):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs)\n",
    "            # Though in the formula, E is negative, it is kept positive here for easier comparison.\n",
    "            # Note that Temperature is greater than 0.\n",
    "            if Temperature <= 0:\n",
    "                raise ValueError(\"Temperature must be greater than 0.\")\n",
    "            energy = Temperature * torch.logsumexp(logits / Temperature, dim=1)\n",
    "            all_scores.append(energy.cpu())\n",
    "    return torch.cat(all_scores)\n",
    "\n",
    "energy_scores = energy_score(model, test_loader, Temperature=1)\n",
    "print(f\"Energy scores: {energy_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93feb73b",
   "metadata": {},
   "source": [
    "# 5. Mahalanobis Distance-based OOD Detection\n",
    "\n",
    "For each class $c$, compute the class mean in feature space:\n",
    "$$ \\mu _c = 1/N_c \\sum_{i:y_i=c}{}{f(x_i)} $$\n",
    "\n",
    "Then, estimate a shared covariance matrix $\\Sigma$ (or per-class covariance), typically eh empirical covariance of features across the training set.\n",
    "\n",
    "Therefore, the Mahalanobis score calculated as:\n",
    "$$ d_{ M a h a } ( x ) = \\operatorname* { m i n } _ { c } \\, ( f ( x ) - \\mu _ { c } ) ^ { \\top } \\Sigma ^ { - 1 } ( f ( x ) - \\mu _ { c } ) $$\n",
    "\n",
    "For each test sample, calculate the distance to all 100 class centers and take the minimum distance as the OOD score (the smaller the distance, the more similar to an in-distribution sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40566874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training features...\n",
      "Computing class means...\n",
      "Computing covariance matrix...\n",
      "Computing Mahalanobis distances for the test set...\n",
      "Mahalanobis scores (first 10): tensor([-467.2338, -358.8292, -417.4225, -505.1868, -386.7666, -281.6877,\n",
      "        -348.1957, -450.8566, -178.3548, -193.7439])\n"
     ]
    }
   ],
   "source": [
    "features_buffer = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # output shape is [batch, 512, 1, 1], flatten it to [batch, 512]\n",
    "    features_buffer.append(output.view(output.size(0), -1))\n",
    "\n",
    "# General hook registration to the top avgpool layer (works for ResNet18/34/50/101)\n",
    "handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "\n",
    "def extract_features(model, inputs):\n",
    "    features_buffer.clear() \n",
    "    _ = model(inputs)       \n",
    "    return features_buffer[0] \n",
    "\n",
    "def mahalanobis_score(model, train_loader, test_loader, num_classes=100):\n",
    "    \"\"\"\n",
    "    Correct implementation of Mahalanobis distance:\n",
    "    1. Compute the mean μ_c for each class (100 classes)\n",
    "    2. Compute the covariance matrix Σ\n",
    "    3. For each test sample, compute the distance to all classes and take the minimum\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 1: Collect training features and labels\n",
    "    print(\"Collecting training features...\")\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            features = extract_features(model, inputs)\n",
    "            train_features.append(features.cpu())\n",
    "            train_labels.append(labels)\n",
    "    \n",
    "    train_features = torch.cat(train_features)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "    \n",
    "    # Step 2: Compute the mean for each class\n",
    "    print(\"Computing class means...\")\n",
    "    class_means = []\n",
    "    for c in range(num_classes):\n",
    "        class_features = train_features[train_labels == c]\n",
    "        class_mean = torch.mean(class_features, dim=0)\n",
    "        class_means.append(class_mean)\n",
    "    class_means = torch.stack(class_means)  # shape: [100, feature_dim]\n",
    "    \n",
    "    # Step 3: Compute the covariance matrix (using global covariance)\n",
    "    print(\"Computing covariance matrix...\")\n",
    "    cov = torch.cov(train_features.T) + 0.01 * torch.eye(train_features.size(1))\n",
    "    inv_cov = torch.inverse(cov)\n",
    "    \n",
    "    # Step 4: Compute the minimum Mahalanobis distance to each class for the test set\n",
    "    print(\"Computing Mahalanobis distances for the test set...\")\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            features = extract_features(model, inputs).cpu()\n",
    "            \n",
    "            # For each sample in the batch\n",
    "            batch_scores = []\n",
    "            for feat in features:\n",
    "                # Compute Mahalanobis distance to each class\n",
    "                dists = []\n",
    "                for class_mean in class_means:\n",
    "                    diff = feat - class_mean\n",
    "                    # Mahalanobis distance: sqrt(diff^T * Σ^-1 * diff). But according to the formula given in the slides,\n",
    "                    # we use the squared distance.\n",
    "                    dist = diff @ inv_cov @ diff\n",
    "                    dists.append(dist.item())\n",
    "                \n",
    "                # Take the minimum distance (smaller distance means more likely to be an ID sample)\n",
    "                min_dist = min(dists)\n",
    "                batch_scores.append(-min_dist)  # Negative sign: higher score means more likely to be ID\n",
    "            \n",
    "            all_scores.append(torch.tensor(batch_scores))\n",
    "\n",
    "    return torch.cat(all_scores)\n",
    "\n",
    "mahalanobis_scores = mahalanobis_score(model, train_loader, test_loader)\n",
    "print(f\"Mahalanobis scores (first 10): {mahalanobis_scores[:10]}\")\n",
    "\n",
    "# Cleanup hook\n",
    "if 'handle' in globals():\n",
    "    handle.remove()\n",
    "    features_buffer.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a212b5",
   "metadata": {},
   "source": [
    "# 6. ViM (Virtual Matching) Score\n",
    "\n",
    "ViM detects OOD samples by analyzing the principal component directions in the feature space:\n",
    "- ID samples' features primarily reside in a low-dimensional subspace (formed by principal components)\n",
    "- OOD samples exhibit larger projections beyond the principal component directions\n",
    "\n",
    "$$s_{ViM}(x) = -\\alpha \\cdot \\|P_{principal}(f(x) - \\mu)\\|_2 + \\|P_{residual}(f(x) - \\mu)\\|_2$$\n",
    "\n",
    "where:\n",
    "- $P_{principal}$: projection onto the principal component subspace\n",
    "- $P_{residual}$: projection onto the residual subspace (orthogonal complement space)\n",
    "- $\\alpha$: weighting parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training features for PCA...\n",
      "Computing feature mean...\n",
      "Performing PCA...\n",
      "Retaining top 100 principal components\n",
      "Explained variance ratio of principal components: 0.9004\n",
      "Computing ViM scores for the test set...\n",
      "ViM scores (first 10): tensor([-6.6161, -5.4149, -7.4246, -6.5089, -9.2754, -5.5998, -5.0616, -7.3689,\n",
      "        -7.2696, -6.6848])\n"
     ]
    }
   ],
   "source": [
    "features_buffer = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # output shape is [batch, 512, 1, 1], flatten it to [batch, 512]\n",
    "    features_buffer.append(output.view(output.size(0), -1))\n",
    "\n",
    "# General hook registration to the top avgpool layer (works for ResNet18/34/50/101)\n",
    "handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "\n",
    "def extract_features(model, inputs):\n",
    "    features_buffer.clear() \n",
    "    _ = model(inputs)       \n",
    "    return features_buffer[0] \n",
    "\n",
    "\n",
    "def vim_score(model, train_loader, test_loader, num_principal_components=100, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ViM OOD detection method.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_principal_components: Number of principal components to retain\n",
    "    - alpha: Weight for the principal component direction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 1: Collect training features\n",
    "    print(\"Collecting training features for PCA...\")\n",
    "    train_features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            features = extract_features(model, inputs)\n",
    "            train_features.append(features.cpu())\n",
    "    \n",
    "    train_features = torch.cat(train_features)\n",
    "    \n",
    "    # Step 2: Compute mean and center features\n",
    "    print(\"Computing feature mean...\")\n",
    "    mean = torch.mean(train_features, dim=0)\n",
    "    centered_features = train_features - mean\n",
    "    \n",
    "    # Step 3: PCA - Compute eigenvalues and eigenvectors of covariance matrix\n",
    "    print(\"Performing PCA...\")\n",
    "    cov = torch.cov(centered_features.T)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
    "    \n",
    "    # Sort by eigenvalue in descending order\n",
    "    idx = torch.argsort(eigenvalues, descending=True)\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Step 4: Select principal components and residual subspace\n",
    "    principal_components = eigenvectors[:, :num_principal_components]  # Principal component directions\n",
    "    residual_components = eigenvectors[:, num_principal_components:]   # Residual directions\n",
    "    \n",
    "    print(f\"Retaining top {num_principal_components} principal components\")\n",
    "    print(f\"Explained variance ratio of principal components: {eigenvalues[:num_principal_components].sum() / eigenvalues.sum():.4f}\")\n",
    "    \n",
    "    # Step 5: Compute ViM scores for the test set\n",
    "    print(\"Computing ViM scores for the test set...\")\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            features = extract_features(model, inputs).cpu()\n",
    "            \n",
    "            # Center the features\n",
    "            centered = features - mean\n",
    "            \n",
    "            # Project onto principal component subspace\n",
    "            principal_proj = centered @ principal_components\n",
    "            principal_norm = torch.norm(principal_proj, dim=1)\n",
    "            \n",
    "            # Project onto residual subspace\n",
    "            residual_proj = centered @ residual_components\n",
    "            residual_norm = torch.norm(residual_proj, dim=1)\n",
    "            \n",
    "            # ViM score: -alpha * ||principal component projection|| + ||residual projection||\n",
    "            # OOD samples have larger components in the residual direction\n",
    "            vim_scores = -alpha * principal_norm + residual_norm\n",
    "            \n",
    "            all_scores.append(vim_scores)\n",
    "    \n",
    "    return torch.cat(all_scores)\n",
    "\n",
    "# Compute ViM scores\n",
    "vim_scores = vim_score(model, train_loader, test_loader, num_principal_components=100, alpha=1.0)\n",
    "print(f\"ViM scores (first 10): {vim_scores[:10]}\")\n",
    "\n",
    "# Cleanup hook\n",
    "if 'handle' in globals():\n",
    "    handle.remove()\n",
    "    features_buffer.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2275ea",
   "metadata": {},
   "source": [
    "# 3. Study the Neural Collapse phenomenon at the end of training NC1 to NC4. \n",
    "\n",
    "Assume that the model is overparameterized.\n",
    "\n",
    "NC1: variability collapse: at the end of training, the in-class variation collapses to very low, due to the feature of each class converges to the mean of the class.\n",
    "\n",
    "NC2: convergence to Simplex Equiangular Tight Frame(ETF): the mean centers of different classes form an ETF structure, meaning they are equidistant from each other in the feature space while maximizing angular separation.\n",
    "\n",
    "NC3: convergence to self-duaality: the class means and linear classifiers converges to each other, up to rescaling.\n",
    "\n",
    "NC4: simplification to nearest-class center: the behavior of the classifier ultimately simplifies to classification based on the nearest class centers in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8e5fe",
   "metadata": {},
   "source": [
    "# 4. Study the Neural Collapse phenomenon at the end of training NC5. \n",
    "\n",
    "NC5: ID/OOD orthogonality: as training progresses, the clusters of OOD data become increasingly orthogonal to the configuration adopted by ID data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255d2ba",
   "metadata": {},
   "source": [
    "# 5. Implementation of the NECO method (Neural Collapse Inspired OOD Detection).\n",
    "\n",
    "$$ NECO(x)=\\frac{||P~h_{\\omega}(x)||}{||h_{\\omega}(x)||} \\\\\n",
    "=\\frac{\\sqrt{h_{\\omega}(x)^\\top PP^\\top h_{\\omega}(x)}}{\\sqrt{h_{\\omega}(x)^\\top h_{\\omega}(x)}}$$\n",
    "\n",
    "With $h_{\\omega}(x)$ the penultimate layer representation and $P$ the projection matrix on the biggest $d$-eigenvectors.\n",
    "\n",
    "$P$ is fitted using a PCA on the in-distribution training features.\n",
    "\n",
    "NECO is rescaled by the maximum-logit value. This has the effect of injecting class-based information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6f58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
